---
title: "Weekly 10 Summary"
author: "Gaurang Kakade"
title-block-banner: true
title-block-style: default
toc: true
#format: html
format: pdf
---

---



## Thursday, March 30th



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1.  Neural Network with 1 Hidden Layer
2.  Neural Network with 2 Hidden Layer
3.  Regression with Neural Network
:::

## Libraries Used

```{r}
library(tidyverse)
library(torch)
library(glmnet)
library(caret)
library(dplyr)
library(tidyr)
library(nnet)
library(rpart)
library(e1071)
```

```{r}
ex1 <- \(x) ifelse(
  sign(x[1] * x[2]) + 0.01 * rnorm(1) <= 0, 0, 1)
  
n <- 200
X <- t(replicate(n, 2 * runif(2) - 1))
y <- apply(X, 1, ex1) %>%
  as.factor()
col <- ifelse(y == 0, 'blue', 'red')
df <- data.frame(y = y, x1 = X[, 1], x2 = X[, 2])
plot(df$x1, df$x2, col = col, pch = 19)

```

```{r}
Xnew <- cbind(
  rep(seq(-1.1, 1.1, length.out = 50), 50),
  rep(seq(-1.1, 1.1, length.out = 50), each = 50)
)

df_new = data.frame(x1 = Xnew[, 1], x2 = Xnew[, 2])

plt <- function(f, x){
  plot(x[,1], x[,2], col = ifelse(f(x) < 0.5, "blue", "red"), pch = 22)
  points(df$x1, df$x2, col = ifelse(y == "0", "blue", "red"), pch = 19)
}

overview <- function(f){
  predicted <- ifelse(f(df[, -1]) < 0.5, 0, 1)
  actual <- df[, 1]
  table(predicted, actual)
}
```

```{r}
f_logistic = \(x){
  glm(y ~ x1 + x2, df, family = binomial()) %>%
    predict(., x, type = "response")
}

plt(f_logistic, df_new)
```

```{r}
f_dtree = \(x){
  rpart(y ~ x1 + x2, df, method = "class") %>%
    predict(., x, type = "class") %>%
    as.numeric(.) - 1
}

plt(f_dtree, df_new)
```

## Neural Network with 1 Hidden Layer

```{r}
module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(2, 20)
    self$g <- nn_linear(20, 1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$s()
  }
)
```

```{r}
#pre-process data
X_tensor <- torch_tensor(df[, -1] %>% as.matrix(), dtype = torch_float())
y_tensor <- torch_tensor(cbind(df[, 1] %>% as.numeric() - 1), dtype = torch_float())
```

```{r}
Loss <- function(x, y, model){
  nn_bce_loss()(model(x), y)
}
```

```{r}
F <- module()
Loss(X_tensor, y_tensor, F)
F <- module()
optimizer <- optim_adam(F$parameters, lr = 0.05)
epochs <- 1000
for(i in 1:epochs){
  loss <- Loss(X_tensor, y_tensor, F)
  
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  if (i < 10 || i %% 100 == 0) {
    cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
  }
}
f_nn = \(x) as_array(F(torch_tensor(x %>% as.matrix(), dtype = torch_float())))


overview(f_logistic)
overview(f_dtree)
```

#### Neural Network with 2 hidden layer

``` r
Xnew <- cbind(
  rep(seq(-1, 1, length.out = 50), 50),
  rep(seq(-1, 1, length.out = 50), each = 50)
)
df_new = data.frame(x1=Xnew[,1], x2=Xnew[,2])
p <- 2
q <- 20
q1 <- 100
q2 <- 20
hh2_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(p,q1)
    self$g <- nn_linear(q1,q2)
    self$h <- nn_linear(q2,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() %>%
      self$s()
  }
)
F <- hh2_module()
optimizer <- optim_adam(F$parameters, lr = 0.05)
epochs <- 1000
for(i in 1:epochs){
  loss <- Loss(X_tensor, y_tensor, F)
  
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  if (i < 10 || i %% 100 == 0) {
    cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
  }
}
f_nn = \(x) as_array(F(torch_tensor(x %>% as.matrix(), dtype = torch_float())))
plot(f_nn, df_new)
f <- classifier(df, 'nn')
plt(f, df_new)
overview(df)
```

``` r
classifier <-
  function(train, type = 'nn', ...) {
    if(type == 'logistic'){
      ######
    }
    elseif(type == 'rpart'){
      ########
    }
    elseif(type == 'svm'){
      ####
    }
    elseif(type == 'nn'){
      #########
    }
  }
```

#### Regression with Neural Networks

```{r}
generate_data <- function(n, noise = 0.1) {
  x <- seq(1*pi, 2*pi, length.out = n)
  y <- exp(x) * (sin(150/x) + rnorm(n, 0, noise))
  data.frame(x = x, y = y)
}
df <- generate_data(200, noise = 0.1)
plot(df$x, df$y, pch=19)
```


```{r}
x_new <- seq(0.9 * pi, 2.1*pi,  length.out = 1000)
df_new <- data.frame(x = x_new)
plt_reg <- function (f, x){
  ynew <- f(x)
  ylim <- range(c(ynew, df$y))
  ylim[1] <- max(c(-800, ylim[1]))
  ylim[2] <- min(c(250, ylim[2]))
  xlim <-range(x)
  plot(df$x, df$y, pch = 22, col = 'red', xlim=xlim, ylim = ylim)
  points(x[,1], ynew, pch=22, type='l')
}  
### Lineaer Regression
f_lm = \(x)
    lm(y ~ x, df) %>%
    predict(., x)
plt_reg(f_lm, df_new)
### Polynomial Regression
f_polynomial = \(x)
    lm(y ~ x + I(x^2) + I(x^3) + I(x^5), df) %>%
    predict(., x)
plt_reg(f_polynomial, df_new)
### Regression Tree
f_dtree = \(x)
    rpart(y ~ x, df) %>%
    predict(., x)
plt_reg(f_dtree, df_new)
### SVM
f_svm = \(x)
    svm(y ~ x, df, kernel = 'radial') %>%
    predict(., x)
plt_reg(f_svm, df_new)
### Neural Network
reg_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(1,20)
    self$g <- nn_linear(20,100)
    self$h <- nn_linear(100,1)
    self$a <- nn_relu()
  # self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() #%>%
    # self$s()
  }
)
f_nn <- function(x){
  F <- reg_module()
  X_tensor <- torch_tensor(df$x %>% as.matrix(), dtype=torch_float())
  y_tensor <- torch_tensor(cbind(df$y), dtype=torch_float())
  optimizer <- optim_adam(F$parameters, lr = 0.05)
  epochs <- 2000
  
  for(i in epochs){
    loss <- nn_mse_loss()(F(X_tensor), y_tensor)
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
  }
  return(as_array(F(torch_tensor(x %>% as.matrix(), dtype=torch_float()))))
} 
plt_reg(f_nn, df_new)
```
