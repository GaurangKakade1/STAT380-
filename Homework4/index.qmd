---
title: "Homework 4"
author: "[Gaurang Kakade]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: html
#format: pdf
---

[Link to the Github repository](https://github.com/psu-stat380/hw-4)

------------------------------------------------------------------------

::: {.callout-important style="font-size: 0.8em;"}
## Due: Sun, Apr 2, 2023 \@ 11:59pm

Please read the instructions carefully before submitting your
assignment.

1.  This assignment requires you to only upload a `PDF` file on Canvas
2.  Don't collapse any code cells before submitting.
3.  Remember to make sure all your code output is rendered properly
    before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter
before submitting your assignment ⚠️
:::

We will be using the following libraries:

```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom"
)

# renv::install(packages)
sapply(packages, require, character.only=T)
```

## <br><br><br><br>

## Question 1

::: callout-tip
## 30 points

Automatic differentiation using `torch`
:::

###### 1.1 (5 points)

Consider $g(x, y)$ given by $$
g(x, y) = (x - 3)^2 + (y - 4)^2.
$$

Using elementary calculus derive the expressions for

$$
\frac{d}{dx}g(x, y), \quad \text{and} \quad \frac{d}{dy}g(x, y).
$$

Using your answer from above, what is the answer to $$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} \quad \text{and} \quad \frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)} ?
$$

Define $g(x, y)$ as a function in R, compute the gradient of $g(x, y)$
with respect to $x=3$ and $y=4$. Does the answer match what you
expected?

```{R}
# Defining the function g(x,y)
g <- function(x,y){
  return((x[1] - 3)^2 + (x[2] - 4)^2)
}
# Computing the gradient of g(x,y) at x = 3 and y = 4
library(numDeriv)
grad_g<-grad(g, c(3,4))
grad_g
```

------------------------------------------------------------------------

###### 1.2 (10 points)

$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$

Consider $h(\u, \v)$ given by $$
h(\u, \v) = (\u \cdot \v)^3,
$$ where $\u \cdot \v$ denotes the dot product of two vectors, i.e.,
$\u \cdot \v = \sum_{i=1}^n u_i v_i.$

Using elementary calculus derive the expressions for the gradients

$$
\begin{aligned}
\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), \dots, \frac{d}{du_n}h(\u, \v)\Bigg)
\end{aligned}
$$

Using your answer from above, what is the answer to
$\nabla_\u h(\u, \v)$ when $n=10$ and

$$
\begin{aligned}
\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
\end{aligned}
$$

Define $h(\u, \v)$ as a function in R, initialize the two vectors $\u$
and $\v$ as `torch_tensor`s. Compute the gradient of $h(\u, \v)$ with
respect to $\u$. Does the answer match what you expected?

```{R}
library(torch)

u <- torch_tensor(c(-1,1,-1,1,-1,1,-1,1,-1,1), requires_grad = TRUE)
v <- torch_tensor(c(-1,-1,-1,-1,-1,1,1,1,1,1), requires_grad = TRUE)

h <- function(u,v){
  (torch_dot(u,v)^3)
}

h_uv <- h(u, v)
h_uv$backward()
u$grad
```

------------------------------------------------------------------------

Yes, the result is in line with my expectations.

###### 1.3 (5 points)

Consider the following function $$
f(z) = z^4 - 6z^2 - 3z + 4
$$

Derive the expression for $$
f'(z_0) = \frac{df}{dz}\Bigg|_{z=z_0}
$$ and evaluate $f'(z_0)$ when $z_0 = -3.5$.

Define $f(z)$ as a function in R, and using the `torch` library compute
$f'(-3.5)$.

```{R}
library(torch)
f <- function(z){
  return(z^4 - 6*z^2 - 3*z + 4)
}

z_0 <- torch_tensor(-3.5, requires_grad = TRUE)
y <- f(z_0)

y$backward()
z_0$grad
```

------------------------------------------------------------------------

###### 1.4 (5 points)

For the same function $f$, initialize $z[1] = -3.5$, and perform $n=100$
iterations of **gradient descent**, i.e.,

> \$z\[{k+1}\] = z\[k\] - \\eta f'(z\[k\]) \\ \\ \\ \\ \$ for
> $k = 1, 2, \dots, 100$

Plot the curve $f$ and add taking $\eta = 0.02$, add the points
$\{z_0, z_1, z_2, \dots z_{100}\}$ obtained using gradient descent to
the plot. What do you observe?

------------------------------------------------------------------------

```{R}
n <- 100
z <- -3.5
eta <- 0.02
z_values <- c(z)

for(i in 1:n){
  df <- 4 * z ^ 3 - 12 * z - 3
  z <- z - eta * df
  
  z_values <- c(z_values, 2)
}

# plotting the curve
x_values <- seq(-4,4, by = 0.01)
y_values <- f(x_values)

df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = f(z_values))

ggplot() + 
  geom_line(data = df_f, aes(x,y), color = "lightblue",    size = 1) + 
  geom_point(data = df_z, aes(x,y), color = "maroon",      size = 3) + 
  ggtitle("Gradient Descent for f(z)") + 
  ylab("z") + 
  xlab("f(z)")
```

> The gradient descent algorithm for the function is depicted in the
> graph that was produced. The gradient descent algorithm's iterative
> results are shown by the maroon dots, while the light blue line
> depicts the curve of the function f(z) and its values are represented
> by the light blue line. The value of z increases while the algorithm
> runs, moving closer to the function's minimum at z not equal to 0.522.
> The maroon dots represent the z values that were determined after each
> algorithm iteration, and we can see that they converge to the minimal
> value.

###### 1.5 (5 points)

Redo the same analysis as **Question 1.4**, but this time using
$\eta = 0.03$. What do you observe? What can you conclude from this
analysis

```{R}
n <- 100
z <- -3.5
eta <- 0.03
z_values <- c(z)

for(i in 1:n){
  df <- 4 * z ^ 3 - 12 * z - 3
  z <- z - eta * df
  
  z_values <- c(z_values, 2)
}

# plotting the curve
x_values <- seq(-4,4, by = 0.01)
y_values <- f(x_values)

df_f <- data.frame(x = x_values, y = y_values)
df_z <- data.frame(x = z_values, y = f(z_values))

ggplot() + 
  geom_line(data = df_f, aes(x,y), color = "lightblue",    size = 1) + 
  geom_point(data = df_z, aes(x,y), color = "maroon",      size = 3) + 
  ggtitle("Gradient Descent for f(z)") + 
  ylab("z") + 
  xlab("f(z)")
```

> The gradient descent algorithm for the function is depicted in the
> graph that was produced. The gradient descent algorithm's iterative
> results are shown by the maroon dots, while the light blue line
> depicts the curve of the function f(z) and its values are represented
> by the light blue line. The value of z increases while the algorithm
> runs, moving closer to the function's minimum at z not equal to 0.522.
> The maroon dots represent the z values that were determined after each
> algorithm iteration, and we can see that they converge to the minimal
> value.
>
> Due to a higher eta value, the path chosen is a little different from
> the previous graph in that the marron dots oscillate before
> convergeing on the minimum (0.03 instead of 0.02). We can observe from
> the analysis that the gradient descent algorithm's performance depends
> on the learning rate that is selected. Slower convergence is the
> result of a lower learning rate, while overshooting and oscillation
> around the minimum are the results of a higher learning rate. We can
> draw the conclusion that it's critical to pick a learning pace that's
> appropriate for the particular issue being tackled.

<br><br><br><br> <br><br><br><br> ---

## Question 2

::: callout-tip
## 50 points

Logistic regression and interpretation of effect sizes
:::

For this question we will use the **Titanic** dataset from the Stanford
data archive. This dataset contains information about passengers aboard
the Titanic and whether or not they survived.

------------------------------------------------------------------------

###### 2.1 (5 points)

Read the data from the following URL as a tibble in R. Preprocess the
data such that the variables are of the right data type, e.g., binary
variables are encoded as factors, and convert all column names to lower
case for consistency. Let's also rename the response variable `Survival`
to `y` for convenience.

```{R}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read.csv(url) %>% 
  drop_na()
# Converting binary variables to factors. Pclass is a categorical variable, but it we can convert it to a factor.  
df <- df %>%
  mutate(Sex = as.factor(Sex),
         Survived = as.factor(Survived))

# Converting all the columns names to lower case for consistency. 
colnames(df) <- tolower(colnames(df))

# Renaming the response variable `Survival` to `y` for convenience. 
df <- df%>%
  rename(y = survived)
```

------------------------------------------------------------------------

###### 2.2 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using
`corrplot()`

```{R}
cor_matrix <- df %>% 
  keep(is.numeric) %>%
  cor()
corrplot(cor_matrix,type = "upper", order = "hclust" )
```

------------------------------------------------------------------------

###### 2.3 (10 points)

Fit a logistic regression model to predict the probability of surviving
the titanic as a function of:

-   `pclass`
-   `sex`
-   `age`
-   `fare`
-   `# siblings`
-   `# parents`

```{R}
full_model <- glm(y ~ pclass + sex + age + fare  + siblings.spouses.aboard + parents.children.aboard, df, family = binomial())
summary(full_model)
```

------------------------------------------------------------------------

###### 2.4 (30 points)

Provide an interpretation for the slope and intercept terms estimated in
`full_model` in terms of the log-odds of survival in the titanic and in
terms of the odds-ratio (if the covariate is also categorical).

The log-odds of survival are the intercept (5.297252), which is the
value when all other variables are set to zero. Due to the fact that it
is a constant element in the model, it cannot be simply interpreted in
terms of survival probability. This coefficient, pclass (-1.177659),
shows how the log-odds of survival alter with each increase in class
level (from 1st to 2nd class or from 2nd to 3rd class). According to
chances ratios, exp(-1.177659) 0.308, which indicates that the
probability of surviving drop by roughly 69.2% for every level of class.

::: callout-hint
## 

Recall the definition of logistic regression from the lecture notes, and
also recall how we interpreted the slope in the linear regression model
(particularly when the covariate was categorical).
:::

<br><br><br><br> <br><br><br><br> ---

## Question 3

::: callout-tip
## 70 points

Variable selection and logistic regression in `torch`
:::

------------------------------------------------------------------------

###### 3.1 (15 points)

Complete the following function `overview` which takes in two
categorical vectors (`predicted` and `expected`) and outputs:

-   The prediction accuracy
-   The prediction error
-   The false positive rate, and
-   The false negative rate

```{R}
overview <- function(predicted, expected){
    total_false_positives <- sum(predicted !=expected & predicted == 1)
    total_true_positives <- sum(predicted == expected & expected == 1)
    total_false_negatives <- sum(predicted != expected & predicted == 0)
    total_true_negatives <- sum(predicted == expected & expected == 0)
    false_positive_rate <- total_false_positives/(total_false_positives + total_true_negatives)
    false_negative_rate <- total_false_negatives/(total_false_negatives + total_true_positives)
    accuracy <- (total_true_positives + total_true_negatives)/length(predicted)
    error <- 1 - accuracy
    return(
        data.frame(
            accuracy = accuracy, 
            error=error, 
            false_positive_rate = false_positive_rate, 
            false_negative_rate = false_negative_rate
        )
    )
}
```

You can check if your function is doing what it's supposed to do by
evaluating

```{R}
overview(df$y, df$y)
```

## and making sure that the accuracy is $100\%$ while the errors are $0\%$.

###### 3.2 (5 points)

Display an overview of the key performance metrics of `full_model`

```{R}
prob_full_model <- predict(full_model, type = "response")
pred_full_model <- ifelse(prob_full_model >= 0.5, 1, 0)

full_model_overview <- overview(prob_full_model, df$y)
full_model_overview
```

------------------------------------------------------------------------

###### 3.3 (5 points)

Using backward-stepwise logistic regression, find a parsimonious
altenative to `full_model`, and print its `overview`

```{R}
step_model <- step(full_model, direction = "backward", scope = formula(full_model))

summary(step_model)
```

```{R}
step_predictions <- predict(step_model, type = "response")
step_predictions <- ifelse(step_predictions >= 0.5, 1, 0)

stepwise_overview <- overview(step_predictions, df$y)
stepwise_overview
```

------------------------------------------------------------------------

###### 3.4 (15 points)

Using the `caret` package, setup a $5$-fold cross-validation training
method using the `caret::trainConrol()` function

```{R}
controls <- trainControl(method = "cv", number = 5)
```

Now, using `control`, perform $5$-fold cross validation using
`caret::train()` to select the optimal $\lambda$ parameter for LASSO
with logistic regression.

Take the search grid for $\lambda$ to be in
$\{ 2^{-20}, 2^{-19.5}, 2^{-19}, \dots, 2^{-0.5}, 2^{0} \}$.

```{R}
# Insert your code in the ... region
lasso_fit <- train(
  y ~ .,
  data = subset(df, select = -name),
  method = "glmnet",
  trControl = controls, 
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = 2^seq(-20, 0, by = 0.5)
    ),
  family = "binomial"
)
```

Using the information stored in `lasso_fit$results`, plot the results
for cross-validation accuracy vs. $log_2(\lambda)$. Choose the optimal
$\lambda^*$, and report your results for this value of $\lambda^*$.

```{R}
# Plotting the results for cross-validation accuracy vs 
# log_2(lambda)
plot(lasso_fit)
```

------------------------------------------------------------------------

```{R}
# Finding the optimal value for lambda
lambda_optimal <- lasso_fit$results$lambda[which.max(lasso_fit$results$Accuracy)]

accuracy_optimal <- max(lasso_fit$results$Accuracy)

cat("The optimal value for lambda is", lambda_optimal, "\n")
cat("The cross-validation accuracy for the optimal lambda is ", accuracy_optimal, "\n")
lasso_predictions <- predict(lasso_fit)
lasso_overview <- overview(lasso_predictions, df$y)
lasso_overview
```

###### 3.5 (25 points)

First, use the `model.matrix()` function to convert the covariates of
`df` to a matrix format

```{R}
covariate_matrix <- model.matrix(full_model)[, -1]
```

Now, initialize the covariates $X$ and the response $y$ as `torch`
tensors

```{R}
X <- torch_tensor(covariate_matrix, dtype = torch_float())
y <- torch_tensor(df$y, dtype = torch_float())
```

Using the `torch` library, initialize an `nn_module` which performs
logistic regression for this dataset. (Remember that we have 6 different
covariates)

```{R}
logistic <- nn_module(
  initialize = function() {
    self$f <- nn_linear(6,1)
    self$g <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
    self$f() %>%
    self$g()
  }
)

f <- logistic()
```

You can verify that your code is right by checking that the output to
the following code is a vector of probabilities:

```{R}
f(X)
```

Now, define the loss function `Loss()` which takes in two tensors `X`
and `y` and a function `Fun`, and outputs the **Binary cross Entropy
loss** between `Fun(X)` and `y`.

```{R}
Loss <- function(X, y, Fun){
  nn_bce_loss()(Fun(X), y)
}
```

Initialize an optimizer using `optim_adam()` and perform $n=1000$ steps
of gradient descent in order to fit logistic regression using `torch`.

```{R}
f <- logistic()
optimizer <- optim_adam(f$parameters, lr=0.01) # Insert your code here

n <- 1000
for (i in 1:n){
    loss <- Loss(X, y, f)
    
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
    
    if (i %% 100 == 0) {
        cat(sprintf("Step %d, Loss: %.4f\n", i, loss))
    }
}
```

Using the final, optimized parameters of `f`, compute the compute the
predicted results on `X`

```{R}
predicted_probabilities <- f(X) %>% as_array()
torch_predictions <- ifelse(predicted_probabilities >= 0.5, 1, 0)

torch_overview <- overview(torch_predictions, df$y)
torch_overview

```

------------------------------------------------------------------------

###### 3.6 (5 points)

Create a summary table of the `overview()` summary statistics for each
of the $4$ models we have looked at in this assignment, and comment on
their relative strengths and drawbacks.

```{R}
combine_overview <- rbind(full_model_overview, stepwise_overview, lasso_overview, torch_overview) %>%
  mutate(Model = c('Full Model', 'Stepwise Regression', 'LASSO', 'Torch'))
  
combine_overview <- combine_overview[, c('Model', 'accuracy', 'error', 'false_positive_rate', 'false_negative_rate')]

combine_overview
```

> Based on the output, we can see that the Stepwise Regression model
> seems to performs slightly better than LASSO model, and far better
> than the remaining two models. LASSO model selects a smaller set of
> predictors and has a lower false positive rate. The Full model
> performs poorly and hence is not suitable for the data given. However,
> it is imperative to note that the relative strengths and weaknesses of
> the models may depend on the specific context and the goals of the
> analysis.

::: {.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br> <br><br><br><br> ---

::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
